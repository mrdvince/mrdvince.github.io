<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Nebo - Build and Deploy(API as a Service) | Nomadin</title>
<meta name=keywords content><meta name=description content="Inspiration
Agriculture is one of the major sources of income in 3rd world countries. In the age of AI, we have seen problems like self-driving take priority amongst other cool research, but I haven&rsquo;t seen a good enough solution that helps the average farmer get better yields, understand what is the problem with their crop. I haven&rsquo;t seen other start-ups utilize the power that deep learning can have and integrate the same in their products and help solve an even bigger, broader scope of problems a farmer and the whole agricultural ecosystem has."><meta name=author content="Vince"><link rel=canonical href=https://mrdvince.github.io/posts/ckvmevbpz0mwg7ls1hbs99s69/><meta name=google-site-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d7e0cf82c7928cf0c779a3f2e91e4634fd283070630f2ad5682f09edbf62aa03.css integrity="sha256-1+DPgseSjPDHeaPy6R5GNP0oMHBjDyrVaC8J7b9iqgM=" rel="preload stylesheet" as=style><link rel=icon href=https://mrdvince.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mrdvince.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mrdvince.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mrdvince.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mrdvince.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mrdvince.github.io/posts/ckvmevbpz0mwg7ls1hbs99s69/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Nebo - Build and Deploy(API as a Service)"><meta property="og:description" content="Inspiration
Agriculture is one of the major sources of income in 3rd world countries. In the age of AI, we have seen problems like self-driving take priority amongst other cool research, but I haven&rsquo;t seen a good enough solution that helps the average farmer get better yields, understand what is the problem with their crop. I haven&rsquo;t seen other start-ups utilize the power that deep learning can have and integrate the same in their products and help solve an even bigger, broader scope of problems a farmer and the whole agricultural ecosystem has."><meta property="og:type" content="article"><meta property="og:url" content="https://mrdvince.github.io/posts/ckvmevbpz0mwg7ls1hbs99s69/"><meta property="og:image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1636101513592/UZZCmv9v-.jpeg?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=compress,format&amp;format=webp"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-11-05T13:12:24+01:00"><meta property="article:modified_time" content="2021-11-05T13:12:24+01:00"><meta property="og:site_name" content="Nomadin"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cdn.hashnode.com/res/hashnode/image/upload/v1636101513592/UZZCmv9v-.jpeg?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=compress,format&amp;format=webp"><meta name=twitter:title content="Nebo - Build and Deploy(API as a Service)"><meta name=twitter:description content="Inspiration
Agriculture is one of the major sources of income in 3rd world countries. In the age of AI, we have seen problems like self-driving take priority amongst other cool research, but I haven&rsquo;t seen a good enough solution that helps the average farmer get better yields, understand what is the problem with their crop. I haven&rsquo;t seen other start-ups utilize the power that deep learning can have and integrate the same in their products and help solve an even bigger, broader scope of problems a farmer and the whole agricultural ecosystem has."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://mrdvince.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Nebo - Build and Deploy(API as a Service)","item":"https://mrdvince.github.io/posts/ckvmevbpz0mwg7ls1hbs99s69/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Nebo - Build and Deploy(API as a Service)","name":"Nebo - Build and Deploy(API as a Service)","description":"Inspiration Agriculture is one of the major sources of income in 3rd world countries. In the age of AI, we have seen problems like self-driving take priority amongst other cool research, but I haven\u0026rsquo;t seen a good enough solution that helps the average farmer get better yields, understand what is the problem with their crop. I haven\u0026rsquo;t seen other start-ups utilize the power that deep learning can have and integrate the same in their products and help solve an even bigger, broader scope of problems a farmer and the whole agricultural ecosystem has.\n","keywords":[],"articleBody":"Inspiration Agriculture is one of the major sources of income in 3rd world countries. In the age of AI, we have seen problems like self-driving take priority amongst other cool research, but I haven‚Äôt seen a good enough solution that helps the average farmer get better yields, understand what is the problem with their crop. I haven‚Äôt seen other start-ups utilize the power that deep learning can have and integrate the same in their products and help solve an even bigger, broader scope of problems a farmer and the whole agricultural ecosystem has.\nFor E.g. a food supply start-up can leverage the knowledge (by using a question answering model fine-tuned with agricultural documents) about the treatment of a certain pathogen and supply pesticides back to the farmer while still getting farm produce from them.\nThis is just one example of a simple use case.\nWhat is Nebo? Nebo is a noun meaning the Babylonian god of wisdom and agriculture and patron of scribes and schools\nThis project combines computer vision and natural language processing to give access to anyone interested in agriculture(from agricultural researchers, farmers, and hobbyists).\nUsing object detection, the prediction and localization of plant disease can be easily detected, and further combining this with question answering model feeding questions about the pathogen, the user gets to learn about what‚Äôs ailing the crops, how to treat them, and a whole range of other agricultural-related questions.\nAll this without having to know much about deep learning, with the provided API endpoints integration with any other application or building one from scratch tailored to one‚Äôs needs is pretty seamless (well, depends on your SWE skills but you can figure it out üòÖ).\nHow it‚Äôs built. Backend The backend uses fastAPI, the reason for choosing this is how intuitive and easy it is to just get things done.\nIt has really good documentation and as an added advantage has a pretty comprehensive cookie-cutter template to get you started on building.\nIts ‚Äúout of the box‚Äù interactive API docs that automatically get updated and allow for direct interaction with the API is a bonus\nDeep learning models used Pathogen detection The object detection model used is a Yolo medium model, medium because the server this is running on is not super powerful so to reduce inference times, apart from using a smaller image size for inference the medium model is also not ‚ÄúCPU heavy‚Äù.\nThe reason for going with detection over classification was to have the pathogen indicated on the image passed, the thinking behind this was it can help if a researcher wants to concentrate on the specific part infected, also providing cropped regions of the regions is an added advantage. All this is made possible using ultralytics YOLOv5 models built with PyTorch (I highly recommend taking a look at their GitHub repository)\nQuestion answering The question answering bit currently comes from OpenAI‚Äôs GPT3.\nThis is mainly for testing purposes and to see how viable using something like GPT3 is compared to training once‚Äôs QA model. An added advantage for using GPT3 (still in beta) is the ease of integration and not having to do much model training. The downside though is the cost (for my case) hence the reason to switch to a distilled Bert model from hugging face (after fine-tuning). Also highly recommend checking out hugging face docs and GitHub repo\nUsing the completions endpoint from the openai docs:\nThe completions endpoint can be used for a wide variety of tasks. It provides a simple but powerful text-in, text-out interface to any of our models. You input some text as a prompt, and the model will generate a text completion that attempts to match whatever context or pattern you gave it.\nPrompt design We first show the model exactly what we want show and tell as openai calls it. This helps the model understand what is expected from it.\nWe then provide quality data to the model showing a given pattern, one reason for this is to have a more ‚Äúintentional response‚Äù.\nThen finally configure the settings, the temperature, and top_p settings control how deterministic the model is in generating a response. In this case, we want a single right answer hence setting these to lower values. Check out the openai docs for further reading\nThe data One major challenge was getting an already existing annotated dataset.\nThis meant building one from scratch by pulling images online and manually annotating them. Using the DuckDuckGo API(using Microsoft‚Äôs Bing Image Search API would have probably been better for this but it‚Äôs paid and am working on a super low budget hence need to find an alternative), I pulled around 200 images per class, initially going for 16 classes of plant diseases but later reduced the number to 8 classes. This is due to the time needed to research a particular pathogen and annotate the images properly while discarding the irrelevant ones.\nAnnotations were done using makesense.ai a free online tool for labeling images built using react.\nOnce bounding boxes are drawn the labels can be downloaded as a zip file containing text files with the label number and the box coordinates.\nFinally split the data into two sets, a training set and a validation set\nimport os import random import shutil imgList = os.listdir(\"images\") random.seed(42) # shuffling images random.shuffle(imgList) split = 0.2 train_path = \"custom_dataset/train\" val_path = \"custom_dataset/val\" if os.path.isdir(train_path) == False: os.makedirs(train_path) if os.path.isdir(val_path) == False: os.makedirs(val_path) imgLen = len(imgList) train_images = imgList[: int(imgLen - (imgLen * split))] val_images = imgList[int(imgLen - (imgLen * split)) :] for imgName in train_images: og_path = os.path.join(\"images\", imgName) target_path = os.path.join(train_path, imgName) shutil.copyfile(og_path, target_path) og_txt_path = os.path.join(\"labels\", imgName.replace(\".JPG\", \".txt\")) target_txt_path = os.path.join(train_path, imgName.replace(\".JPG\", \".txt\")) try: shutil.copyfile(og_txt_path, target_txt_path) except Exception: pass for imgName in val_images: og_path = os.path.join(\"images\", imgName) target_path = os.path.join(val_path, imgName) shutil.copyfile(og_path, target_path) og_txt_path = os.path.join(\"labels\", imgName.replace(\".JPG\", \".txt\")) target_txt_path = os.path.join(val_path, imgName.replace(\".JPG\", \".txt\")) try: shutil.copyfile(og_txt_path, target_txt_path) except Exception: pass print(\"Done! \") Once this is done, I went ahead and uploaded the whole dataset to Weights \u0026 Biases as an artifact.\nWeights \u0026 Biases artifacts make dataset and model versioning straightforward, tables to go through the data including evaluation comparisons for prediction without requiring extra code.\nAn added advantage to pushing the data to Weights \u0026 Biases is the fact that now you can train your model from any computer, the dataset will be automatically pulled to the machine and all the run logs are automatically pushed to the project on the W\u0026B.\nTraining the model The model was trained using a single GPU(one v100 to be specific) instance from using GCP(Google cloud platform), using the below line\npython train.py --data bocr/app/data/boc_labels_wandb.yaml --epochs 300 --project box_of_crayons --bbox_interval 1 --save_period 30 --weights yolov5m6.pt --batch 40 --artifact_alias v2 --cache --hyp bocr/app/trainer/data/hyps/hyp.scratch-p6.yaml As seen passing the boc_labels_wandb.yaml as the data path uses the dataset from Weights \u0026 Biases.\n# the boc_labels_wandb.yaml names: - Apple Scab - Apple Cedar Rust - Apple Frogeye Spot - Maize Gray Leaf Spot - Maize Leaf Blight - Potato Blight - Tomato Bacteria Spot - Tomato Blight nc: 8 train: wandb-artifact://box_of_crayons/train val: wandb-artifact://box_of_crayons/val Logging metrics with Weights \u0026 Biases One advantage of using the yolov5 is it‚Äôs already setup to log metrics using W\u0026B, see this GitHub issue to learn more about it.\nIn a nutshell, the following run information is streamed to the W\u0026B cloud console while the training job runs:\nTraining \u0026 Validation losses Metrics: Precision, Recall, mAP@0.5, mAP@0.5:0.95 Learning Rate over time A bounding box debugging panel, showing the training progress over time GPU: Type, GPU Utilization, power, temperature, CUDA memory usage System: Disk I/0, CPU utilization, RAM usage Your trained model as W\u0026B Artifact Environment: OS and Python types, Git repository, and state, training command Deployment The dependencies for deployment are docker and docker-compose.\nDocker can be installed by running\n# Download Docker curl -fsSL get.docker.com -o get-docker.sh # Install Docker using the stable channel (instead of the default \"edge\") CHANNEL=stable sh get-docker.sh # Remove Docker install script rm get-docker.sh and docker-compose\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose Currently, these are the only dependencies unless you want to deploy the containers using Kubernetes(probably overkill but still fun to try out), an article for another time.\nSelf-hosted container registry (optional) In my case, I also set up a self-hosted container registry.\nThe reason for this was to store most of the images built to use these with Kubernetes and rancher, these can also be replaced in the compose files with the images from the registry once they have been built.\nThe reason for not using the docker hub to host the images is the price constraint as well since the docker hub charges for privately hosted docker images. (and yes $5 per month is a lot depending on financial factors).\nSo a workaround for this is using an AWS free tier EC2 instance and combining that with S3 buckets. If you have AWS credits(which I happen to have) these can cover the cost of some of the limits exceeded.\nCheck my repo on hosting your own registry here.\nPush to the instance using GitHub actions All those are housed on a cheap VPS from contabo(4 core, 8Gb ram(yes it gets cheaper than digital ocean)), and thanks to GitHub actions the deployment to the VM can be automated.\nThis is achieved by using a GitHub action that executes remote SSH commands. (How secure this is when a repo is made public, I am not sure but using GitHub secrets might help a little bit with that). Check out this link to learn more.\nWhat‚Äôs next for Nebo? Keep improving the current dataset, the better the quality of the data the better the model performance. Do full documentation on the API, making it easy for other developers to use it. Try out new stripe integration (should be interesting) Keep trying out new things and keep experimenting. ","wordCount":"1646","inLanguage":"en","image":"https://cdn.hashnode.com/res/hashnode/image/upload/v1636101513592/UZZCmv9v-.jpeg?w=1600\u0026h=840\u0026fit=crop\u0026crop=entropy\u0026auto=compress,format\u0026format=webp","datePublished":"2021-11-05T13:12:24+01:00","dateModified":"2021-11-05T13:12:24+01:00","author":{"@type":"Person","name":"Vince"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mrdvince.github.io/posts/ckvmevbpz0mwg7ls1hbs99s69/"},"publisher":{"@type":"Organization","name":"Nomadin","logo":{"@type":"ImageObject","url":"https://mrdvince.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mrdvince.github.io/ accesskey=h title="Nomadin (Alt + H)">Nomadin</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mrdvince.github.io/archives/ title=Blog><span>Blog</span></a></li><li><a href=https://mrdvince.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Nebo - Build and Deploy(API as a Service)</h1><div class=post-meta><span title='2021-11-05 13:12:24 +0100 CET'>November 5, 2021</span>&nbsp;¬∑&nbsp;8 min&nbsp;¬∑&nbsp;1646 words&nbsp;¬∑&nbsp;Vince</div></header><figure class=entry-cover><img loading=eager src="https://cdn.hashnode.com/res/hashnode/image/upload/v1636101513592/UZZCmv9v-.jpeg?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=compress,format&amp;format=webp" alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#backend>Backend</a></li><li><a href=#deep-learning-models-used>Deep learning models used</a></li></ul></li><li><a href=#the-data>The data</a></li><li><a href=#training-the-model>Training the model</a><ul><li><a href=#logging-metrics-with-weights--biases>Logging metrics with Weights & Biases</a></li></ul></li></ul><ul><li><a href=#self-hosted-container-registry-optional>Self-hosted container registry (optional)</a></li><li><a href=#push-to-the-instance-using-github-actions>Push to the instance using GitHub actions</a></li></ul></nav></div></details></div><div class=post-content><h1 id=inspiration>Inspiration<a hidden class=anchor aria-hidden=true href=#inspiration>#</a></h1><p>Agriculture is one of the major sources of income in 3rd world countries. In the age of AI, we have seen problems like self-driving take priority amongst other cool research, but I haven&rsquo;t seen a good enough solution that helps the average farmer get better yields, understand what is the problem with their crop. I haven&rsquo;t seen other start-ups utilize the power that deep learning can have and integrate the same in their products and help solve an even bigger, broader scope of problems a farmer and the whole agricultural ecosystem has.</p><p>For E.g. a food supply start-up can leverage the knowledge (by using a question answering model fine-tuned with agricultural documents) about the treatment of a certain pathogen and supply pesticides back to the farmer while still getting farm produce from them.</p><p>This is just one example of a simple use case.</p><h1 id=what-is-nebo>What is Nebo?<a hidden class=anchor aria-hidden=true href=#what-is-nebo>#</a></h1><blockquote><p>Nebo is a noun meaning the Babylonian god of wisdom and agriculture and patron of scribes and schools</p></blockquote><p>This project combines computer vision and natural language processing to give access to anyone interested in agriculture(from agricultural researchers, farmers, and hobbyists).</p><p>Using object detection, the prediction and localization of plant disease can be easily detected, and further combining this with question answering model feeding questions about the pathogen, the user gets to learn about what&rsquo;s ailing the crops, how to treat them, and a whole range of other agricultural-related questions.</p><p>All this without having to know much about deep learning, with the provided API endpoints integration with any other application or building one from scratch tailored to one&rsquo;s needs is pretty seamless (well, depends on your SWE skills but you can figure it out üòÖ).</p><h1 id=how-its-built>How it&rsquo;s built.<a hidden class=anchor aria-hidden=true href=#how-its-built>#</a></h1><h3 id=backend>Backend<a hidden class=anchor aria-hidden=true href=#backend>#</a></h3><p>The backend uses fastAPI, the reason for choosing this is how intuitive and easy it is to just get things done.</p><p>It has really good documentation and as an added advantage has a pretty comprehensive cookie-cutter template to get you started on building.</p><p>Its &ldquo;out of the box&rdquo; interactive API docs that automatically get updated and allow for direct interaction with the API is a bonus</p><h3 id=deep-learning-models-used>Deep learning models used<a hidden class=anchor aria-hidden=true href=#deep-learning-models-used>#</a></h3><h4 id=pathogen-detection>Pathogen detection<a hidden class=anchor aria-hidden=true href=#pathogen-detection>#</a></h4><p>The object detection model used is a Yolo medium model, medium because the server this is running on is not super powerful so to reduce inference times, apart from using a smaller image size for inference the medium model is also not &ldquo;CPU heavy&rdquo;.</p><p>The reason for going with detection over classification was to have the pathogen indicated on the image passed, the thinking behind this was it can help if a researcher wants to concentrate on the specific part infected, also providing cropped regions of the regions is an added advantage.
All this is made possible using <em>ultralytics YOLOv5</em> models built with PyTorch (I highly recommend taking a look at their <a href=https://github.com/ultralytics/yolov5>GitHub repository</a>)</p><h4 id=question-answering>Question answering<a hidden class=anchor aria-hidden=true href=#question-answering>#</a></h4><p>The question answering bit currently comes from OpenAI&rsquo;s GPT3.</p><blockquote><p>This is mainly for testing purposes and to see how viable using something like GPT3 is compared to training once&rsquo;s QA model. An added advantage for using GPT3 (still in beta) is the ease of integration and not having to do much model training.
The downside though is the cost (for my case) hence the reason to switch to a distilled Bert model from hugging face (after fine-tuning).
Also highly recommend checking out <a href=https://huggingface.co/transformers/index.html>hugging face docs</a>
and <a href=https://github.com/huggingface/transformers>GitHub repo</a></p></blockquote><p>Using the completions endpoint from the openai docs:</p><blockquote><p>The completions endpoint can be used for a wide variety of tasks. It provides a simple but powerful text-in, text-out interface to any of our models. You input some text as a prompt, and the model will generate a text completion that attempts to match whatever context or pattern you gave it.</p></blockquote><h4 id=prompt-design>Prompt design<a hidden class=anchor aria-hidden=true href=#prompt-design>#</a></h4><p>We first show the model exactly what we want <code>show and tell</code> as openai calls it. This helps the model understand what is expected from it.</p><p>We then provide quality data to the model showing a given pattern, one reason for this is to have a more &ldquo;intentional response&rdquo;.</p><p>Then finally configure the settings, the temperature, and top_p settings control how deterministic the model is in generating a response. In this case, we want a single right answer hence setting these to lower values.
Check out the <a href=https://beta.openai.com/docs/guides/completion/prompt-design>openai docs</a> for further reading</p><h2 id=the-data>The data<a hidden class=anchor aria-hidden=true href=#the-data>#</a></h2><p>One major challenge was getting an already existing annotated dataset.</p><p>This meant building one from scratch by pulling images online and manually annotating them.
Using the DuckDuckGo API(using Microsoft‚Äôs Bing Image Search API would have probably been better for this but it&rsquo;s paid and am working on a super low budget hence need to find an alternative), I pulled around 200 images per class, initially going for 16 classes of plant diseases but later reduced the number to 8 classes. This is due to the time needed to research a particular pathogen and annotate the images properly while discarding the irrelevant ones.</p><p>Annotations were done using makesense.ai a free online tool for labeling images built using react.</p><p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1636113263034/oFn9548Kn.png alt=image.png>
Once bounding boxes are drawn the labels can be downloaded as a zip file containing text files with the label number and the box coordinates.</p><p>Finally split the data into two sets, a training set and a validation set</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>shutil</span>
</span></span><span class=line><span class=cl><span class=n>imgList</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>listdir</span><span class=p>(</span><span class=s2>&#34;images&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># shuffling images</span>
</span></span><span class=line><span class=cl><span class=n>random</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>imgList</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>split</span> <span class=o>=</span> <span class=mf>0.2</span>
</span></span><span class=line><span class=cl><span class=n>train_path</span> <span class=o>=</span> <span class=s2>&#34;custom_dataset/train&#34;</span>
</span></span><span class=line><span class=cl><span class=n>val_path</span> <span class=o>=</span> <span class=s2>&#34;custom_dataset/val&#34;</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>isdir</span><span class=p>(</span><span class=n>train_path</span><span class=p>)</span> <span class=o>==</span> <span class=kc>False</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>os</span><span class=o>.</span><span class=n>makedirs</span><span class=p>(</span><span class=n>train_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>isdir</span><span class=p>(</span><span class=n>val_path</span><span class=p>)</span> <span class=o>==</span> <span class=kc>False</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>os</span><span class=o>.</span><span class=n>makedirs</span><span class=p>(</span><span class=n>val_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>imgLen</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>imgList</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>train_images</span> <span class=o>=</span> <span class=n>imgList</span><span class=p>[:</span> <span class=nb>int</span><span class=p>(</span><span class=n>imgLen</span> <span class=o>-</span> <span class=p>(</span><span class=n>imgLen</span> <span class=o>*</span> <span class=n>split</span><span class=p>))]</span>
</span></span><span class=line><span class=cl><span class=n>val_images</span> <span class=o>=</span> <span class=n>imgList</span><span class=p>[</span><span class=nb>int</span><span class=p>(</span><span class=n>imgLen</span> <span class=o>-</span> <span class=p>(</span><span class=n>imgLen</span> <span class=o>*</span> <span class=n>split</span><span class=p>))</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>imgName</span> <span class=ow>in</span> <span class=n>train_images</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>og_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=s2>&#34;images&#34;</span><span class=p>,</span> <span class=n>imgName</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>target_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>train_path</span><span class=p>,</span> <span class=n>imgName</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>shutil</span><span class=o>.</span><span class=n>copyfile</span><span class=p>(</span><span class=n>og_path</span><span class=p>,</span> <span class=n>target_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>og_txt_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=s2>&#34;labels&#34;</span><span class=p>,</span> <span class=n>imgName</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s2>&#34;.JPG&#34;</span><span class=p>,</span> <span class=s2>&#34;.txt&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>target_txt_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>train_path</span><span class=p>,</span> <span class=n>imgName</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s2>&#34;.JPG&#34;</span><span class=p>,</span> <span class=s2>&#34;.txt&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>shutil</span><span class=o>.</span><span class=n>copyfile</span><span class=p>(</span><span class=n>og_txt_path</span><span class=p>,</span> <span class=n>target_txt_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>Exception</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>pass</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>imgName</span> <span class=ow>in</span> <span class=n>val_images</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>og_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=s2>&#34;images&#34;</span><span class=p>,</span> <span class=n>imgName</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>target_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>val_path</span><span class=p>,</span> <span class=n>imgName</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>shutil</span><span class=o>.</span><span class=n>copyfile</span><span class=p>(</span><span class=n>og_path</span><span class=p>,</span> <span class=n>target_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>og_txt_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=s2>&#34;labels&#34;</span><span class=p>,</span> <span class=n>imgName</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s2>&#34;.JPG&#34;</span><span class=p>,</span> <span class=s2>&#34;.txt&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>target_txt_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>val_path</span><span class=p>,</span> <span class=n>imgName</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s2>&#34;.JPG&#34;</span><span class=p>,</span> <span class=s2>&#34;.txt&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>shutil</span><span class=o>.</span><span class=n>copyfile</span><span class=p>(</span><span class=n>og_txt_path</span><span class=p>,</span> <span class=n>target_txt_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>Exception</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>pass</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Done! &#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>Once this is done, I went ahead and uploaded the whole dataset to <a href=https://wandb.ai>Weights & Biases</a> as an artifact.</p><p>Weights & Biases artifacts make dataset and model versioning straightforward, tables to go through the data including evaluation comparisons for prediction without requiring extra code.</p><p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1636114144015/CpyKuOs9l.png alt=image.png></p><p>An added advantage to pushing the data to Weights & Biases is the fact that now you can train your model from any computer, the dataset will be automatically pulled to the machine and all the run logs are automatically pushed to the project on the W&amp;B.</p><h2 id=training-the-model>Training the model<a hidden class=anchor aria-hidden=true href=#training-the-model>#</a></h2><p>The model was trained using a single GPU(one v100 to be specific) instance from
using GCP(Google cloud platform), using the below line</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python train.py --data bocr/app/data/boc_labels_wandb.yaml --epochs <span class=m>300</span> --project box_of_crayons --bbox_interval <span class=m>1</span> --save_period <span class=m>30</span> --weights yolov5m6.pt --batch <span class=m>40</span> --artifact_alias v2 --cache --hyp bocr/app/trainer/data/hyps/hyp.scratch-p6.yaml
</span></span></code></pre></div><p>As seen passing the <code>boc_labels_wandb.yaml</code> as the data path uses the dataset from Weights & Biases.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># the boc_labels_wandb.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>names</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>Apple Scab</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>Apple Cedar Rust</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>Apple Frogeye Spot</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>Maize Gray Leaf Spot</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>Maize Leaf Blight</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>Potato Blight</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>Tomato Bacteria Spot</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>Tomato Blight</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>nc</span><span class=p>:</span><span class=w> </span><span class=m>8</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>train</span><span class=p>:</span><span class=w> </span><span class=l>wandb-artifact://box_of_crayons/train</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>val</span><span class=p>:</span><span class=w> </span><span class=l>wandb-artifact://box_of_crayons/val</span><span class=w>
</span></span></span></code></pre></div><h3 id=logging-metrics-with-weights--biases>Logging metrics with Weights & Biases<a hidden class=anchor aria-hidden=true href=#logging-metrics-with-weights--biases>#</a></h3><p>One advantage of using the yolov5 is it&rsquo;s already setup to log metrics using W&amp;B,
see this <a href=https://github.com/ultralytics/yolov5/issues/1289>GitHub issue</a> to learn more about it.</p><p>In a nutshell, the following run information is streamed to the W&amp;B cloud console while the training job runs:</p><ul><li>Training & Validation losses</li><li>Metrics: Precision, Recall, <a href=mailto:mAP@0.5>mAP@0.5</a>, <a href=mailto:mAP@0.5>mAP@0.5</a>:0.95</li><li>Learning Rate over time</li><li>A bounding box debugging panel, showing the training progress over time</li><li>GPU: Type, GPU Utilization, power, temperature, CUDA memory usage</li><li>System: Disk I/0, CPU utilization, RAM usage</li><li>Your trained model as W&amp;B Artifact</li><li>Environment: OS and Python types, Git repository, and state, training command</li></ul><h1 id=deployment>Deployment<a hidden class=anchor aria-hidden=true href=#deployment>#</a></h1><p>The dependencies for deployment are docker and docker-compose.</p><p>Docker can be installed by running</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Download Docker</span>
</span></span><span class=line><span class=cl>curl -fsSL get.docker.com -o get-docker.sh
</span></span><span class=line><span class=cl><span class=c1># Install Docker using the stable channel (instead of the default &#34;edge&#34;)</span>
</span></span><span class=line><span class=cl><span class=nv>CHANNEL</span><span class=o>=</span>stable sh get-docker.sh
</span></span><span class=line><span class=cl><span class=c1># Remove Docker install script</span>
</span></span><span class=line><span class=cl>rm get-docker.sh
</span></span></code></pre></div><p>and docker-compose</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo curl -L <span class=s2>&#34;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-</span><span class=k>$(</span>uname -s<span class=k>)</span><span class=s2>-</span><span class=k>$(</span>uname -m<span class=k>)</span><span class=s2>&#34;</span> -o /usr/local/bin/docker-compose
</span></span></code></pre></div><p>Currently, these are the only dependencies unless you want to deploy the containers using Kubernetes(probably overkill but still fun to try out), an article for another time.</p><h2 id=self-hosted-container-registry-optional>Self-hosted container registry (optional)<a hidden class=anchor aria-hidden=true href=#self-hosted-container-registry-optional>#</a></h2><p>In my case, I also set up a self-hosted container registry.</p><p>The reason for this was to store most of the images built to use these with Kubernetes and rancher, these can also be replaced in the compose files with the images from the registry once they have been built.</p><p>The reason for not using the docker hub to host the images is the price constraint as well since the docker hub charges for privately hosted docker images. (and yes $5 per month is a lot depending on financial factors).</p><p>So a workaround for this is using an AWS free tier EC2 instance and combining that with S3 buckets. If you have AWS credits(which I happen to have) these can cover the cost of some of the limits exceeded.</p><p>Check <a href=https://github.com/mrdvince/docker_self_hosted_registry>my repo</a> on hosting your own registry <a href=https://github.com/mrdvince/docker_self_hosted_registry>here.</a></p><h2 id=push-to-the-instance-using-github-actions>Push to the instance using GitHub actions<a hidden class=anchor aria-hidden=true href=#push-to-the-instance-using-github-actions>#</a></h2><p>All those are housed on a cheap VPS from contabo(4 core, 8Gb ram(yes it gets cheaper than digital ocean)), and thanks to GitHub actions the deployment to the VM can be automated.</p><p>This is achieved by using a GitHub action that executes remote SSH commands. (How secure this is when a repo is made public, I am not sure but using GitHub secrets might help a little bit with that).
Check out this <a href=https://github.com/appleboy/ssh-action>link</a> to learn more.</p><h1 id=whats-next-for-nebo>What&rsquo;s next for Nebo?<a hidden class=anchor aria-hidden=true href=#whats-next-for-nebo>#</a></h1><ol><li>Keep improving the current dataset, the better the quality of the data the better the model performance.</li><li>Do full documentation on the API, making it easy for other developers to use it.</li><li>Try out new stripe integration (should be interesting)</li><li>Keep trying out new things and keep experimenting.</li></ol></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://mrdvince.github.io/posts/ckx8pdli806axz7s10sx5fy8c/><span class=title>¬´ Prev</span><br><span>Mime - A dummy Django, PyTorch project</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://mrdvince.github.io/>Nomadin</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>