[{"content":"Installing Rust Getting Rust installed on your system can be done by following the guide from the official website https://www.rust-lang.org/tools/install.\nAs of writing this if installing on a Mac, Linux or WSL the recommended way is running this curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh on your terminal.\nVerify the installation by opening the terminal and typing rustc --version. This should show the version of Rust installed on your system.\nrustc --version rustc 1.67.0 (fc594f156 2023-01-24) Note: Rust is supported on various platforms such as Windows, macOS, Linux, and BSD.\nCargo What is Cargo? Cargo is the go-to package manager for Rust development. It simplifies the management of dependencies, building, and testing of projects, as well as the sharing of code libraries by publishing to crates.io - the public registry for Rust packages.\nCreating a Rust Project with Cargo To create a new Rust project with Cargo, follow these steps:\nOpen the terminal and navigate to the directory where you want to create your project. Type the command cargo new \u0026lt;project_name\u0026gt; to create a new Rust project with Cargo. Change into the project directory using cd \u0026lt;project_name\u0026gt;. # cargo new hello hello ├── Cargo.toml └── src └── main.rs Once you have created a new Rust project with Cargo, you will see two main files in the project directory:\nCargo.toml: This file is the configuration file for your project. It contains information about your project, such as its name, version, and dependencies. You can add or remove dependencies as needed for your project by editing this file.\n# Cargo.toml [package] name = \u0026#34;hello\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html [dependencies] main.rs: This is the main Rust file for your project. This file contains the entry point for your Rust program.\n// main.rs fn main() { println!(\u0026#34;Hello, world!\u0026#34;); } That\u0026rsquo;s it! You now have a basic Rust project up and running using Cargo.\nRunning this is as straight forward as just typing cargo run on the console.\ncargo run Compiling hello v0.1.0 (...) Finished dev [unoptimized + debuginfo] target(s) in 0.41s Running `target/debug/hello` Hello, world! Not this shows that cargo is running a dev build to build a release version add --release to the cargo run command and you get an optimized release version.\ncargo run --release Variables Variables in Rust are declared using the \u0026rsquo;let\u0026rsquo; keyword followed by the name of the variable, and an equals sign to initialize the value.\nNOTE: Variables are immutable by default\nMutable and Immutable Variables In Rust, variables can be either mutable or immutable. Mutable variables are declared using the let mut keyword, while immutable variables are declared using just the let keyword.\nExample of Mutable Variables:\nlet mut x = 5; println!(\u0026#34;The value of x is: {}\u0026#34;, x); x = 10; println!(\u0026#34;The value of x after mutation is: {}\u0026#34;, x); Example of Immutable Variables:\nlet y = 5; println!(\u0026#34;The value of y is: {}\u0026#34;, y); y = 10; // This line would throw a compile error because y is immutable Constants Constants in Rust are variables whose value cannot be changed once set. They are declared using the \u0026lsquo;const\u0026rsquo; keyword.\nExample of Constants:\nconst MAX_POINTS: u32 = 100_000; println!(\u0026#34;The value of MAX_POINTS is: {}\u0026#34;, MAX_POINTS); MAX_POINTS = 50_000; // This line throws a compile error because MAX_POINTS is a constant SIDENOTE: Rust\u0026rsquo;s helpful error messages One of the things I like so far is the helpful error messages that explain what\u0026rsquo;s going wrong in the code. e.g in the previous case when declaring variables, if you make a mistake, Rust provides an error message that explains the issue and how to fix it.\nFor example, in the below compiler message\nerror[E0384]: cannot assign twice to immutable variable `y` --\u0026gt; src/main.rs:29:5 | 27 | let y = 5; | - | | | first assignment to `y` | help: consider making this binding mutable: `mut y` 28 | println!(\u0026#34;The value of y is: {}\u0026#34;, y); 29 | y = 10; | ^^^^^^ cannot assign twice to immutable variable when trying to assign a value to an immutable variable, we not only get the error message but also a help section to add the mut keyword to y (pretty neat if you ask me).\nScopes In Rust, scoping refers to the visibility and accessibility of variables and other identifiers within a program.\nOne way to create a scope in Rust is by using curly braces {}. Anything defined within the curly braces will only be accessible within that scope. For example:\nfn main() { let x = 5; { let y = 10; println!(\u0026#34;x: {} y: {}\u0026#34;, x, y); } println!(\u0026#34;x: {}\u0026#34;, x); println!(\u0026#34;y: {}\u0026#34;, y); } In this code snippet, the variable x is defined in the main scope and is accessible within the whole function. The variable y is defined within the inner scope created by the curly braces and is not accessible outside of it. The second println statement for \u0026ldquo;y\u0026rdquo; will result in an error because y is not defined in the main scope.\nAnother way to create scope in Rust is by using functions. The variables defined within a function are only accessible within that function. For example:\nfn main() { let x = 5; let y = scope(); println!(\u0026#34;x: {} y: {}\u0026#34;, x, y); } fn scope() -\u0026gt; i32 { let y = 10; y } In this code snippet, the variable y is defined within the scope of the scope function and is not accessible outside of it. If we try to print y in the main function, the program will not compile because y is not defined in the main scope.\nA bit out of the \u0026ldquo;scope\u0026rdquo; of fundamentals :)\nWhen a variable goes out of scope, the memory that was allocated for it is freed. This process is automatically handled by Rust\u0026rsquo;s memory management system.\nOne of the key features of Rust\u0026rsquo;s memory management is that it ensures that a variable\u0026rsquo;s memory is only freed when the variable goes out of scope and is no longer accessible. This is known as deterministic destruction (I need to learn more about this) and it prevents common bugs such as use-after-free and double-free errors.\nFor example, in the following code snippet:\nfn main() { let x = vec![1, 2, 3]; { let y = vec![4, 5, 6]; println!(\u0026#34;x: {:?} y: {:?}\u0026#34;, x, y); } println!(\u0026#34;x: {:?}\u0026#34;, x); } The variable y goes out of scope when the inner curly braces close. At this point, the memory allocated for y is freed and can be used for other purposes. However, the variable x is still accessible within the main scope, so its memory is not freed.\nMemory safety Some of the features that make Rust a safe language:\nstrong type checking automatic memory management and the language was designed with a focus on safety from the ground up. The language\u0026rsquo;s ownership model and borrow checker ensure that memory is always properly managed and cannot be accidentally accessed after it has been freed.\nThe compiler guarantees these at compile, e,g in the following examples:\nUninitialized Variables: let x: i32; println!(\u0026#34;The value of x is: {}\u0026#34;, x); // we get error[E0381]: used binding `x` isn\u0026#39;t initialized --\u0026gt; src/main.rs:23:39 | 22 | let x: i32; | - binding declared here but left uninitialized 23 | println!(\u0026#34;The value of x is: {}\u0026#34;, x); | ^ `x` used here but it isn\u0026#39;t initialized | = note: this error originates in the macro `$crate::format_args_nl` which comes from the expansion of the macro `println` (in Nightly builds, run with -Z macro-backtrace for more info) help: consider assigning a value | 22 | let x: i32 = 0; | +++ In the above example, x is uninitialized and the rust compiler tells us we should consider assigning a value to it and it recomments an example let x: i32 = 0;\nOwnership and Borrowing: let mut s = String::from(\u0026#34;hello\u0026#34;); let r1 = s; let r2 = s; println!(\u0026#34;{} and {}\u0026#34;, r1, r2); s.push_str(\u0026#34;, world\u0026#34;); In the above example, s is a mutable String. Attempting to assign r2 to s (which would usually work in a language like python) leads to a compiler error value used here after move. This section requires a whole page in itself so for now the rust docs will do.\nAlternatively following the compiler\u0026rsquo;s suggestions does fix our problem for now.\nDereferencing Null Pointers: use std::ptr::null; let x = null; let y = x.deref(); error[E0699]: the type of this value must be known to call a method on a raw pointer on it --\u0026gt; src/main.rs:23:15 | 23 | let y = x.deref(); | ^^^^^ In this example, x is declared as a null pointer, which means it doesn\u0026rsquo;t point to any valid memory. Attempting to dereference x will result in a compile-time error, preventing the use of a null pointer.\nUsing modules in Rust The Rust module system allows developers to organize their code into different modules, which can be stored in separate files. The use keyword is used to bring an item from a module into the current scope. Think of import in python.\nFor example:\n// In a module called \u0026#39;my_module\u0026#39; pub fn my_function() { println!(\u0026#34;Hello from my_module!\u0026#34;); } // In the main.rs file use my_module::my_function; fn main() { my_function(); } Note: Modules in Rust are private by default, to make them accessible outside their scope, the pub keyword must be added.\nTo import dependencies, we use Cargo. To add a new dependency, you can specify it in the Cargo.toml file, and then use the use keyword to bring it into the current scope.\nFor example, to use the rand crate from crates.io, you add the following to your Cargo.toml file:\n[dependencies] rand = \u0026#34;0.7.3\u0026#34; Then in your Rust code, use the use keyword to import the items from the rand crate:\nuse rand::Rng; fn main() { let mut rng = rand::thread_rng(); println!(\u0026#34;Random number: {}\u0026#34;, rng.gen::\u0026lt;i32\u0026gt;()); } P:S Crates.io is the public registry for Rust packages (kind of like pypi if you come from a python background), where you can find and download libraries and dependencies.\n","permalink":"https://mrdvince.github.io/posts/cldhwjgi3000209ld6fgh6j0l/","summary":"\u003ch1 id=\"installing-rust\"\u003eInstalling Rust\u003c/h1\u003e\n\u003cp\u003eGetting Rust installed on your system can be done by following the guide from the official website \u003ca href=\"https://www.rust-lang.org/tools/install\"\u003ehttps://www.rust-lang.org/tools/install\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAs of writing this if installing on a Mac, Linux or WSL the recommended way is running this \u003ccode\u003ecurl --proto '=https' --tlsv1.2 -sSf\u003c/code\u003e \u003ca href=\"https://sh.rustup.rs\"\u003e\u003ccode\u003ehttps://sh.rustup.rs\u003c/code\u003e\u003c/a\u003e \u003ccode\u003e| sh\u003c/code\u003e on your terminal.\u003c/p\u003e\n\u003cp\u003eVerify the installation by opening the terminal and typing \u003ccode\u003erustc --version\u003c/code\u003e. This should show the version of Rust installed on your system.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003erustc --version\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003erustc 1.67.0 \u003cspan class=\"o\"\u003e(\u003c/span\u003efc594f156 2023-01-24\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote: Rust is supported on various platforms such as Windows, macOS, Linux, and BSD.\u003c/p\u003e","title":"Rust Basics: Taking the First Steps"},{"content":"What is this? This is me goofing around and trying to learn some Django the only way there is, (by attempting to build something and combining it with a bunch of other things I know or have come across).\nAbout This is a Django-based \u0026ldquo;web app\u0026rdquo; that uses a Densenet 121 model trained to classify 6 classes of objects.\nThe Classes Potholes Dumping Accidents Flooded Bad drainage Construction Data The data was got from Duckduck go using jmd_imagecraper\nSteps to replicate Install jmd_imagescraper. This is an image scraping library for creating deep learning datasets. Follow (this link)[https://github.com/joedockrill/jmd_imagescraper] to check out the Github repo.\nImports and download directory\nfrom jmd_imagescraper.core import * from pathlib import Path root = Path().cwd() / \u0026#34;images\u0026#34; This imports and the necessary packages needed and sets the image download directory to the images directory.\nThe directory gets automatically created so no need to worry about that.\nGetting some data These are the classes I was interested in at the time (for no reason at all, so no why) but can be used to get different image classes, just swap out the image names. mime = { \u0026#39;Potholes\u0026#39;:\u0026#39;road potholes\u0026#39;, \u0026#39;Dumping\u0026#39;:\u0026#39;dump sites littering\u0026#39;, \u0026#39;Accidents\u0026#39;:\u0026#39;road accident\u0026#39;, \u0026#39;Flooded\u0026#39;:\u0026#39;flooded roads\u0026#39;, \u0026#39;Bad drainage\u0026#39;:\u0026#39;bad drainages roads\u0026#39;, \u0026#34;Construction\u0026#34;, \u0026#34;Road under construction, construction sites\u0026#34;, } for key, value in mime.items(): duckduckgo_search(root, key, value, max_results=120) The duckduckgo_search function takes in a directory, the name of the image folder, the value to search for and how many images do you want. Then searches for the images and downloads them, making using torchvision\u0026rsquo;s image folder a breeze, no writing custom data loaders, etc (unless you want to).\nData cleaning Another cool feature from this is the image cleaner. Just load the images and go through the classes and remove the images that don\u0026rsquo;t meet your criteria. Makes getting rid of unwanted images pretty easy. from jmd_imagescraper.imagecleaner import * display_image_cleaner(root) Training As this was trained on a simple small dataset (don\u0026rsquo;t get me wrong, the performance is pretty good), the generalization might not be that great.\nNow with that out of the way let\u0026rsquo;s look at some code snips.\nThe training repo All this can be found in this repo.\nSimply follow the getting started part of the README and you should be up running.\nThe data loader Since the different image classes are stored in their respective folders, we just use the folder names as the classes. Using torchvision\u0026rsquo;s image folder method makes this pretty straightforward.\n# The base class can be swapped with the `torch.utils.data.DataLoader` class class AjimeDataLoader(BaseDataLoader): \u0026#34;\u0026#34;\u0026#34;AjimeDataLoader\u0026#34;\u0026#34;\u0026#34; def __init__( self, data_dir, batch_size, shuffle=True, validation_split=0.2, num_workers=4, training=True, ): trsfm = transforms.Compose( [ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] ), ] ) self.data_dir = data_dir self.dataset = datasets.ImageFolder(data_dir, transform=trsfm) super().__init__( self.dataset, batch_size, shuffle, validation_split, num_workers ) Load the data from the data directory, apply some transforms, and voila. Using a validation split of 20% percent of the data for the validation steps.\nIf using a CPU with less than 4 cores just set the num_workers to the number of your CPU cores.\nThe model The model used is a pretrained Densenet 121 model.\nWhy Densenet121? It\u0026rsquo;s never disappointed me, it\u0026rsquo;s small and pretty straightforward to work with. (Note: they are other better model architectures)\n# Densenet Model class AjimeModel(BaseModel): def __init__(self): super().__init__() self.model = torchvision.models.densenet121(pretrained=True) for param in self.model.features.parameters(): param.requires_grad = False self.model.classifier = nn.Sequential( nn.Linear(1024, 512), nn.Dropout(0.2), nn.ReLU(), nn.Linear(512, 6) ) def forward(self, x): return self.model(x) torchvision.models.densenet121(pretrained=True) is responsible for downloading the model.\nThen freeze the feature parameters, and replace the classifier with an output of 6 classes, and we are ready to train.\nThe training loop The training loop is just the usual vanilla PyTorch training loop as seen in this snip.\nfor batch_idx, (data, target) in enumerate(pbar): data, target = data.to(self.device), target.to(self.device) self.optimizer.zero_grad() output = self.model(data) loss = self.criterion(output, target) loss.backward() self.optimizer.step() self.writer.set_step((epoch - 1) * self.len_epoch + batch_idx) self.train_metrics.update(\u0026#34;loss\u0026#34;, loss.item()) for met in self.metric_ftns: self.train_metrics.update(met.__name__, met(output, target)) if batch_idx % self.log_step == 0: pbar.set_postfix( { \u0026#34;Train Epoch\u0026#34;: epoch, \u0026#34;Train Loss\u0026#34;: loss.item(), } ) self.writer.add_image( \u0026#34;input\u0026#34;, make_grid(data.cpu(), nrow=8, normalize=True) ) if batch_idx == self.len_epoch: break Some training logs:\n2021-12-12 12:49:45,955 - trainer - INFO - epoch : 14 2021-12-12 12:49:45,956 - trainer - INFO - loss : 0.2260482641203063 2021-12-12 12:49:45,956 - trainer - INFO - accuracy : 0.9285714285714286 2021-12-12 12:49:45,957 - trainer - INFO - top_k_acc : 1.0 2021-12-12 12:49:45,957 - trainer - INFO - val_loss : 0.8062299564480782 2021-12-12 12:49:45,958 - trainer - INFO - val_accuracy : 0.78125 2021-12-12 12:49:45,958 - trainer - INFO - val_top_k_acc : 0.953125 2021-12-12 12:49:46,146 - trainer - INFO - Saving checkpoint: saved/models/ajime/1212_124412/checkpoint-epoch14.pth ... 2021-12-12 12:49:46,390 - trainer - INFO - Saving current best: model_best.pth ... Deployment All the code for this is also in this repo simply follow the README to get you set up.\nUsing cookie-cutter Django for this was a bit overkill.\nGetting started Dependencies Docker and docker-compose For windows download docker desktop and that should get sorted out.\n# Linux # Download Docker curl -fsSL get.docker.com -o get-docker.sh # Install Docker using the stable channel (instead of the default \u0026#34;edge\u0026#34;) CHANNEL=stable sh get-docker.sh # Remove Docker install script rm get-docker.sh # Docker compose sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose Clone the repository (if you haven\u0026rsquo;t already) Using recursive to include the submodule repo in the cloned directory.\ngit clone --recursive https://github.com/mrdvince/mime cd mime Running it Once cloned is done simply use docker to run it and everything should just work. The model is included in the ajime submodule, can be retrained or completely swapped out if needed.\n# This should get you up and running docker-compose -f local.yml up -d --build Settings Setting Up Your Users\nTo create a normal user account, just go to Sign Up and fill out\nTo create an superuser account, use this command:\npython manage.py createsuperuser Some screenshots ","permalink":"https://mrdvince.github.io/posts/ckx8pdli806axz7s10sx5fy8c/","summary":"\u003ch1 id=\"what-is-this\"\u003eWhat is this?\u003c/h1\u003e\n\u003cp\u003eThis is me goofing around and trying to learn some Django the only way there is, (by attempting to build something and combining it with a bunch of other things I know or have come across).\u003c/p\u003e\n\u003ch1 id=\"about\"\u003eAbout\u003c/h1\u003e\n\u003cp\u003eThis is a Django-based \u0026ldquo;web app\u0026rdquo; that uses a Densenet 121 model trained to classify 6 classes of objects.\u003c/p\u003e\n\u003ch2 id=\"the-classes\"\u003eThe Classes\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePotholes\u003c/li\u003e\n\u003cli\u003eDumping\u003c/li\u003e\n\u003cli\u003eAccidents\u003c/li\u003e\n\u003cli\u003eFlooded\u003c/li\u003e\n\u003cli\u003eBad drainage\u003c/li\u003e\n\u003cli\u003eConstruction\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"data\"\u003eData\u003c/h1\u003e\n\u003cp\u003eThe data was got from Duckduck go using jmd_imagecraper\u003c/p\u003e","title":"Mime - A dummy Django, PyTorch project"},{"content":"Inspiration Agriculture is one of the major sources of income in 3rd world countries. In the age of AI, we have seen problems like self-driving take priority amongst other cool research, but I haven\u0026rsquo;t seen a good enough solution that helps the average farmer get better yields, understand what is the problem with their crop. I haven\u0026rsquo;t seen other start-ups utilize the power that deep learning can have and integrate the same in their products and help solve an even bigger, broader scope of problems a farmer and the whole agricultural ecosystem has.\nFor E.g. a food supply start-up can leverage the knowledge (by using a question answering model fine-tuned with agricultural documents) about the treatment of a certain pathogen and supply pesticides back to the farmer while still getting farm produce from them.\nThis is just one example of a simple use case.\nWhat is Nebo? Nebo is a noun meaning the Babylonian god of wisdom and agriculture and patron of scribes and schools\nThis project combines computer vision and natural language processing to give access to anyone interested in agriculture(from agricultural researchers, farmers, and hobbyists).\nUsing object detection, the prediction and localization of plant disease can be easily detected, and further combining this with question answering model feeding questions about the pathogen, the user gets to learn about what\u0026rsquo;s ailing the crops, how to treat them, and a whole range of other agricultural-related questions.\nAll this without having to know much about deep learning, with the provided API endpoints integration with any other application or building one from scratch tailored to one\u0026rsquo;s needs is pretty seamless (well, depends on your SWE skills but you can figure it out 😅).\nHow it\u0026rsquo;s built. Backend The backend uses fastAPI, the reason for choosing this is how intuitive and easy it is to just get things done.\nIt has really good documentation and as an added advantage has a pretty comprehensive cookie-cutter template to get you started on building.\nIts \u0026ldquo;out of the box\u0026rdquo; interactive API docs that automatically get updated and allow for direct interaction with the API is a bonus\nDeep learning models used Pathogen detection The object detection model used is a Yolo medium model, medium because the server this is running on is not super powerful so to reduce inference times, apart from using a smaller image size for inference the medium model is also not \u0026ldquo;CPU heavy\u0026rdquo;.\nThe reason for going with detection over classification was to have the pathogen indicated on the image passed, the thinking behind this was it can help if a researcher wants to concentrate on the specific part infected, also providing cropped regions of the regions is an added advantage. All this is made possible using ultralytics YOLOv5 models built with PyTorch (I highly recommend taking a look at their GitHub repository)\nQuestion answering The question answering bit currently comes from OpenAI\u0026rsquo;s GPT3.\nThis is mainly for testing purposes and to see how viable using something like GPT3 is compared to training once\u0026rsquo;s QA model. An added advantage for using GPT3 (still in beta) is the ease of integration and not having to do much model training. The downside though is the cost (for my case) hence the reason to switch to a distilled Bert model from hugging face (after fine-tuning). Also highly recommend checking out hugging face docs and GitHub repo\nUsing the completions endpoint from the openai docs:\nThe completions endpoint can be used for a wide variety of tasks. It provides a simple but powerful text-in, text-out interface to any of our models. You input some text as a prompt, and the model will generate a text completion that attempts to match whatever context or pattern you gave it.\nPrompt design We first show the model exactly what we want show and tell as openai calls it. This helps the model understand what is expected from it.\nWe then provide quality data to the model showing a given pattern, one reason for this is to have a more \u0026ldquo;intentional response\u0026rdquo;.\nThen finally configure the settings, the temperature, and top_p settings control how deterministic the model is in generating a response. In this case, we want a single right answer hence setting these to lower values. Check out the openai docs for further reading\nThe data One major challenge was getting an already existing annotated dataset.\nThis meant building one from scratch by pulling images online and manually annotating them. Using the DuckDuckGo API(using Microsoft’s Bing Image Search API would have probably been better for this but it\u0026rsquo;s paid and am working on a super low budget hence need to find an alternative), I pulled around 200 images per class, initially going for 16 classes of plant diseases but later reduced the number to 8 classes. This is due to the time needed to research a particular pathogen and annotate the images properly while discarding the irrelevant ones.\nAnnotations were done using makesense.ai a free online tool for labeling images built using react.\nOnce bounding boxes are drawn the labels can be downloaded as a zip file containing text files with the label number and the box coordinates.\nFinally split the data into two sets, a training set and a validation set\nimport os import random import shutil imgList = os.listdir(\u0026#34;images\u0026#34;) random.seed(42) # shuffling images random.shuffle(imgList) split = 0.2 train_path = \u0026#34;custom_dataset/train\u0026#34; val_path = \u0026#34;custom_dataset/val\u0026#34; if os.path.isdir(train_path) == False: os.makedirs(train_path) if os.path.isdir(val_path) == False: os.makedirs(val_path) imgLen = len(imgList) train_images = imgList[: int(imgLen - (imgLen * split))] val_images = imgList[int(imgLen - (imgLen * split)) :] for imgName in train_images: og_path = os.path.join(\u0026#34;images\u0026#34;, imgName) target_path = os.path.join(train_path, imgName) shutil.copyfile(og_path, target_path) og_txt_path = os.path.join(\u0026#34;labels\u0026#34;, imgName.replace(\u0026#34;.JPG\u0026#34;, \u0026#34;.txt\u0026#34;)) target_txt_path = os.path.join(train_path, imgName.replace(\u0026#34;.JPG\u0026#34;, \u0026#34;.txt\u0026#34;)) try: shutil.copyfile(og_txt_path, target_txt_path) except Exception: pass for imgName in val_images: og_path = os.path.join(\u0026#34;images\u0026#34;, imgName) target_path = os.path.join(val_path, imgName) shutil.copyfile(og_path, target_path) og_txt_path = os.path.join(\u0026#34;labels\u0026#34;, imgName.replace(\u0026#34;.JPG\u0026#34;, \u0026#34;.txt\u0026#34;)) target_txt_path = os.path.join(val_path, imgName.replace(\u0026#34;.JPG\u0026#34;, \u0026#34;.txt\u0026#34;)) try: shutil.copyfile(og_txt_path, target_txt_path) except Exception: pass print(\u0026#34;Done! \u0026#34;) Once this is done, I went ahead and uploaded the whole dataset to Weights \u0026amp; Biases as an artifact.\nWeights \u0026amp; Biases artifacts make dataset and model versioning straightforward, tables to go through the data including evaluation comparisons for prediction without requiring extra code.\nAn added advantage to pushing the data to Weights \u0026amp; Biases is the fact that now you can train your model from any computer, the dataset will be automatically pulled to the machine and all the run logs are automatically pushed to the project on the W\u0026amp;B.\nTraining the model The model was trained using a single GPU(one v100 to be specific) instance from using GCP(Google cloud platform), using the below line\npython train.py --data bocr/app/data/boc_labels_wandb.yaml --epochs 300 --project box_of_crayons --bbox_interval 1 --save_period 30 --weights yolov5m6.pt --batch 40 --artifact_alias v2 --cache --hyp bocr/app/trainer/data/hyps/hyp.scratch-p6.yaml As seen passing the boc_labels_wandb.yaml as the data path uses the dataset from Weights \u0026amp; Biases.\n# the boc_labels_wandb.yaml names: - Apple Scab - Apple Cedar Rust - Apple Frogeye Spot - Maize Gray Leaf Spot - Maize Leaf Blight - Potato Blight - Tomato Bacteria Spot - Tomato Blight nc: 8 train: wandb-artifact://box_of_crayons/train val: wandb-artifact://box_of_crayons/val Logging metrics with Weights \u0026amp; Biases One advantage of using the yolov5 is it\u0026rsquo;s already setup to log metrics using W\u0026amp;B, see this GitHub issue to learn more about it.\nIn a nutshell, the following run information is streamed to the W\u0026amp;B cloud console while the training job runs:\nTraining \u0026amp; Validation losses Metrics: Precision, Recall, mAP@0.5, mAP@0.5:0.95 Learning Rate over time A bounding box debugging panel, showing the training progress over time GPU: Type, GPU Utilization, power, temperature, CUDA memory usage System: Disk I/0, CPU utilization, RAM usage Your trained model as W\u0026amp;B Artifact Environment: OS and Python types, Git repository, and state, training command Deployment The dependencies for deployment are docker and docker-compose.\nDocker can be installed by running\n# Download Docker curl -fsSL get.docker.com -o get-docker.sh # Install Docker using the stable channel (instead of the default \u0026#34;edge\u0026#34;) CHANNEL=stable sh get-docker.sh # Remove Docker install script rm get-docker.sh and docker-compose\nsudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose Currently, these are the only dependencies unless you want to deploy the containers using Kubernetes(probably overkill but still fun to try out), an article for another time.\nSelf-hosted container registry (optional) In my case, I also set up a self-hosted container registry.\nThe reason for this was to store most of the images built to use these with Kubernetes and rancher, these can also be replaced in the compose files with the images from the registry once they have been built.\nThe reason for not using the docker hub to host the images is the price constraint as well since the docker hub charges for privately hosted docker images. (and yes $5 per month is a lot depending on financial factors).\nSo a workaround for this is using an AWS free tier EC2 instance and combining that with S3 buckets. If you have AWS credits(which I happen to have) these can cover the cost of some of the limits exceeded.\nCheck my repo on hosting your own registry here.\nPush to the instance using GitHub actions All those are housed on a cheap VPS from contabo(4 core, 8Gb ram(yes it gets cheaper than digital ocean)), and thanks to GitHub actions the deployment to the VM can be automated.\nThis is achieved by using a GitHub action that executes remote SSH commands. (How secure this is when a repo is made public, I am not sure but using GitHub secrets might help a little bit with that). Check out this link to learn more.\nWhat\u0026rsquo;s next for Nebo? Keep improving the current dataset, the better the quality of the data the better the model performance. Do full documentation on the API, making it easy for other developers to use it. Try out new stripe integration (should be interesting) Keep trying out new things and keep experimenting. ","permalink":"https://mrdvince.github.io/posts/ckvmevbpz0mwg7ls1hbs99s69/","summary":"\u003ch1 id=\"inspiration\"\u003eInspiration\u003c/h1\u003e\n\u003cp\u003eAgriculture is one of the major sources of income in 3rd world countries. In the age of AI, we have seen problems like self-driving take priority amongst other cool research, but I haven\u0026rsquo;t seen a good enough solution that helps the average farmer get better yields, understand what is the problem with their crop. I haven\u0026rsquo;t seen other start-ups utilize the power that deep learning can have and integrate the same in their products and help solve an even bigger, broader scope of problems a farmer and the whole agricultural ecosystem has.\u003c/p\u003e","title":"Nebo - Build and Deploy(API as a Service)"},{"content":"","permalink":"https://mrdvince.github.io/about/","summary":"","title":""}]